{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Loading.......ok~\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import numpy\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import jsonlines\n",
    "import re\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "nlp = spacy.load(os.environ.get('SPACY_MODEL', 'en'))\n",
    "\n",
    "print('Loading.......ok~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_bert_spacy_tokens(spacy_tokens, bert_tokens):\n",
    "    # rules for bert\n",
    "    \n",
    "    # rule (1)\n",
    "    # \"##\" tokens starts with ## means continue words\n",
    "    # like christmas will be split into ['ch','##rist','##mas' ]\n",
    "    \n",
    "    # rule (2)\n",
    "    # bert unknown words is [UNK]\n",
    "    \n",
    "    #lower both tokens\n",
    "    spacy_tokens = [token.lower() for token in spacy_tokens]\n",
    "    bert_tokens = [token.lower() for token in bert_tokens]\n",
    "    \n",
    "    max_idx = len(spacy_tokens)\n",
    "    spacy_idx = 0\n",
    "    alignments = []\n",
    "    \n",
    "    spacy_word_start = 0\n",
    "    \n",
    "    for idx in range(len(bert_tokens)):\n",
    "        \n",
    "        # UNK case\n",
    "        if bert_tokens[idx] == \"[unk]\":\n",
    "            if spacy_word_start == len(spacy_tokens[spacy_idx]):\n",
    "                spacy_word_start = 0\n",
    "                spacy_idx += 1\n",
    "            alignments.append(spacy_idx)\n",
    "            continue\n",
    "        \n",
    "        if spacy_idx == max_idx:\n",
    "            alignments.append(spacy_idx)\n",
    "            continue\n",
    "        \n",
    "        # continue word case\n",
    "        if bert_tokens[idx].startswith(\"##\"):\n",
    "            token = bert_tokens[idx][2:]\n",
    "            tgt_idx = spacy_tokens[spacy_idx][spacy_word_start:].find(token)\n",
    "            spacy_word_start += tgt_idx\n",
    "            alignments.append(spacy_idx)\n",
    "        else:\n",
    "            token = bert_tokens[idx]\n",
    "            tgt_idx = spacy_tokens[spacy_idx][spacy_word_start:].find(token)\n",
    "            while tgt_idx == -1 and spacy_idx != max_idx:\n",
    "                spacy_word_start = 0\n",
    "                spacy_idx += 1\n",
    "                tgt_idx = spacy_tokens[spacy_idx][spacy_word_start:].find(token)\n",
    "            \n",
    "            if tgt_idx != -1:\n",
    "                spacy_word_start += tgt_idx + len(token)\n",
    "            alignments.append(spacy_idx)\n",
    "                \n",
    "    return alignments\n",
    "\n",
    "def load_senses(filename):\n",
    "    senses = defaultdict(lambda :defaultdict(lambda :defaultdict(lambda: dict())))\n",
    "    \n",
    "    for line in open(filename):\n",
    "        try:\n",
    "            word, pos, example, level, vector = line.strip().split(\"\\t\")\n",
    "            vector = np.array([float(value) for value in vector.split()])\n",
    "\n",
    "            senses[word][pos][example]['vector'] = vector\n",
    "            senses[word][pos][example]['level'] = level\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(line[:100])\n",
    "            print(len(line.strip().split(\"\\t\")))\n",
    "\n",
    "    \n",
    "    return senses\n",
    "\n",
    "senses = load_senses(\"output_lv.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd_bert(words, tgt_words_index, pos_tags, lemmatizer, bert):\n",
    "    sentence = \" \".join(words).lower()\n",
    "    \n",
    "    tgt_pos_tags = [pos_tags[idx] for idx in tgt_words_index]\n",
    "#     print(tgt_pos_tags)\n",
    "    lemmas = [lemmatizer(words[idx], pos_tag)[0] \n",
    "              for idx, pos_tag in zip(tgt_words_index, tgt_pos_tags)]\n",
    "    \n",
    "    model, tokenizer = bert\n",
    "    tokenized_text = tokenizer.tokenize(\" \".join(words))\n",
    "    \n",
    "    bertID2spacyID = alignment_bert_spacy_tokens(words, tokenized_text)\n",
    "    spacyID2bertID = {}\n",
    "    for idx, ID in enumerate(bertID2spacyID):\n",
    "        if ID not in spacyID2bertID:\n",
    "            spacyID2bertID[ID] = idx\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "    segments_ids = [0 for _ in range(len(indexed_tokens) )]\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    # If you have a GPU, put everything on cuda\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    segments_tensors = segments_tensors.to('cuda')\n",
    "    model.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for spacyID, lemma, pos_tag in zip(tgt_words_index, lemmas, tgt_pos_tags):\n",
    "        bertID = spacyID2bertID[spacyID]\n",
    "        \n",
    "        embedding = []\n",
    "        for i in range(-1, -5, -1):\n",
    "            embedding.append(encoded_layers[i][0,bertID,])\n",
    "        \n",
    "        embedding = torch.cat(embedding).cpu().numpy()\n",
    "        cosine_values = [ (sentence, sentence_info['level'], cosine_similarity([sentence_info['vector'] ], [embedding])[0][0])\n",
    "                        for sentence, sentence_info in senses[lemma][pos_tag].items()]\n",
    "        \n",
    "        result.append(cosine_values)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd_level(sentence, word):\n",
    "\n",
    "    lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "    bert = (model, tokenizer)\n",
    "    doc = nlp(sentence.lower())\n",
    "    words = []\n",
    "    pos_tags = []\n",
    "    for token in doc:\n",
    "        words.append(token.text)\n",
    "        pos_tags.append(token.pos_)\n",
    "    \n",
    "    idx = [i for i, w in enumerate(words) if w == word]\n",
    "    \n",
    "    wsd_results = wsd_bert(words ,idx, pos_tags, lemmatizer=lemmatizer, bert=bert)\n",
    "#     print(wsd_results)\n",
    "    wsd_scores = [score for example_info in wsd_results for example, level, score in example_info]\n",
    "    wsd_levels = [level for example_info in wsd_results for example, level, score in example_info]\n",
    "\n",
    "#     print(wsd_scores.index(max(wsd_scores)))\n",
    "    return wsd_levels[wsd_scores.index(max(wsd_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_level('you are a pig' , 'pig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aes",
   "language": "python",
   "name": "aes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
